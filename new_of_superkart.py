# -*- coding: utf-8 -*-
"""New of SuperKart.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h-0Ix8LM7u6rZUSER1vWOQiwUYL_sHxb

**Project: Model Deployment: SuperKart**

**Business Context**

A sales forecast predicts future sales revenue based on historical data, industry trends, and the status of the current sales pipeline. Businesses use the sales forecast to estimate weekly, monthly, quarterly, and annual sales totals. A company needs to make an accurate sales forecast as it adds value across an organization and helps the different verticals to chalk out their future course of action.

Forecasting helps an organization plan its sales operations by region and provides valuable insights to the supply chain team regarding the procurement of goods and materials. An accurate sales forecast process has many benefits, which include improved decision-making about the future and the reduction of sales pipeline and forecast risks. Moreover, it helps to reduce the time spent in planning territory coverage and establishes benchmarks that can be used to assess trends in the future.
Objective
SuperKart is a retail chain operating supermarkets and food marts across various tier cities, offering a wide range of products. To optimize its inventory management and make informed decisions around regional sales strategies, SuperKart wants to accurately forecast the sales revenue of its outlets for the upcoming quarter.

To operationalize these insights at scale, the company has partnered with a data science firm, not just to build a predictive model based on historical sales data but also to develop and deploy a robust forecasting solution that can be integrated into SuperKartâ€™s decision-making systems and used across its network of stores.

**Data Dictionary**

The data contains the different attributes of the various products and stores.

Product_Id: Unique identifier of each product, each identifier having two letters at the beginning, followed by a number
Product_Weight: Weight of each product
Product_Sugar_Content: Sugar content of each product, like low sugar, regular, and no sugar
Product_Allocated_Area: Ratio of the allocated display area of each product to the total display area of all the products in a store
Product_Type: Broad category for each product like meat, snack foods, hard drinks, dairy, canned, soft drinks, health and hygiene, baking goods, bread, breakfast, frozen foods, fruits and vegetables, household, seafood, starchy foods, others
Product_MRP: Maximum retail price of each product
Store_Id: Unique identifier of each store
Store_Establishment_Year: Year in which the store was established
Store_Size: Size of the store, depending on sq. feet, like high, medium, and low
Store_Location_City_Type: Type of city in which the store is located, like Tier 1, Tier 2, and Tier 3. Tier 1 consists of cities where the standard of living is comparatively higher than that of its Tier 2 and Tier 3 counterparts
Store_Type: Type of store depending on the products that are being sold there, like Departmental Store, Supermarket Type 1, Supermarket Type 2, and Food Mart
Product_Store_Sales_Total: Total revenue generated by the sale of that particular product in that particular store

**Installing and Importing the necessary libraries**
"""

import subprocess
import sys

packages = [
    "numpy==1.26.4",   # âœ… works with Python 3.13
    "pandas==2.2.2",
    "scikit-learn==1.5.2",  # âœ… latest compatible
    "matplotlib==3.9.2",
    "seaborn==0.13.2",
    "joblib==1.4.2",
    "xgboost==2.1.4",
    "requests==2.32.3",
    "huggingface_hub==0.30.1"
]

import warnings
warnings.filterwarnings("ignore")

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# For splitting the dataset
from sklearn.model_selection import train_test_split

# Libaries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 100)


# Libraries different ensemble classifiers
from sklearn.ensemble import (
    BaggingRegressor,
    RandomForestRegressor,
    AdaBoostRegressor,
    GradientBoostingRegressor,
)
from xgboost import XGBRegressor
from sklearn.tree import DecisionTreeRegressor

# Libraries to get different metric scores
from sklearn.metrics import (
    confusion_matrix,
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    mean_squared_error,
    mean_absolute_error,
    r2_score,
    mean_absolute_percentage_error
)

# To create the pipeline
from sklearn.compose import make_column_transformer
from sklearn.pipeline import make_pipeline,Pipeline

# To tune different models and standardize
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler,OneHotEncoder

# To serialize the model
import joblib

# os related functionalities
import os

# API request
import requests

# for hugging face space authentication to upload files
from huggingface_hub import login, HfApi

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.compose import make_column_transformer
from sklearn import metrics

"""**Loading the dataset**"""

import os
import pandas as pd

# Check if the file exists in Google Drive and use that path if it does
DATA_PATH = r"/content/drive/MyDrive/GREAT LEARNING/Module_7/SuperKart.csv"

if not os.path.exists(DATA_PATH):
    # If not found in Google Drive, raise an error or handle as needed
    raise FileNotFoundError(f"Dataset not found at {DATA_PATH}. Please ensure the file is in your Google Drive.")

df = pd.read_csv(DATA_PATH)

"""**Data Overview**"""

print('\nData shape:', df.shape)

df.head()

print('Columns and types:\n')
print(df.dtypes)

# checking for duplicate values
df.duplicated().sum()

# checking for missing values
df.isnull().sum()

"""**Exploratory Data Analysis (EDA)**

**Feature Engineering - Product Sugar Content**
"""

df['Product_Sugar_Content'].value_counts()

df['Product_Sugar_Content'] = df['Product_Sugar_Content'].replace('reg', 'Regular')

df['Product_Sugar_Content'].value_counts()

# Define the target variable for the regression task
target = 'Product_Store_Sales_Total' # total revenue generated by the sale of that particular product in that particular store

# List of numerical features in the dataset (excluding 'id' as it is an identifier)
numeric_features = [
    'Product_Weight',         # weight of each product
    'Product_Allocated_Area', # ratio of the allocated display area of each product to the total display area of all the products in a store
    'Product_MRP', # maximum retail price of each product
    'Store_Establishment_Year',  # year in which the store was established
]

# List of categorical features in the dataset
categorical_features = [
    'Product_Sugar_Content',  # sugar content of each product like low sugar, regular and no sugar
    'Product_Type',   # broad category for each product like meat, snack foods, hard drinks, dairy etc.
    'Store_Id',          # unique identifier of each store
    'Store_Size',       # size of the store depending on sq. feet like high, medium and low
    'Store_Location_City_Type', #type of city in which the store is located like Tier 1, Tier 2 and Tier 3
    'Store_Type' #type of store depending on the products that are being sold there
]

"""**Univariate Analysis**"""

# Generate summary statistics for numerical features
df[numeric_features].describe()

# Generate summary statistics for categorical features
df[categorical_features].describe()

# Generate summary statistics for the target variable
df[target].describe()

"""**Creating Labeled Barplots for Categorical Columns**"""

# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot

labeled_barplot(df, 'Product_Sugar_Content', perc=True)

labeled_barplot(df, 'Product_Type', perc=True)

labeled_barplot(df, 'Store_Id', perc=True)

labeled_barplot(df, 'Store_Size', perc=True)

labeled_barplot(df, 'Store_Location_City_Type', perc=True)

labeled_barplot(df, 'Store_Type', perc=True)

"""**Creating Histogram for Numerical Columns**"""

sns.histplot(df['Product_Weight'], bins=10)

sns.histplot(df['Product_Allocated_Area'], bins=10)

sns.histplot(df['Product_MRP'], bins=10)

sns.histplot(df['Store_Establishment_Year'])

sns.histplot(df['Product_Store_Sales_Total'], bins=10)

"""**Bivariate Analysis**

**Target variable vs Numerical Columns**
"""

# Checking correlation between numerical columns
plt.figure(figsize=(15, 7))
sns.heatmap(df.corr(numeric_only = True), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral")
plt.show()

"""**Target variables vs Categorical Columns**"""

sns.boxplot(x='Product_Sugar_Content', y='Product_Store_Sales_Total', data=df)

plt.figure(figsize=(12, 6))  # Wider plot gives more space for labels
sns.boxplot(x='Product_Type', y='Product_Store_Sales_Total', data=df)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

sns.boxplot(x='Store_Id', y='Product_Store_Sales_Total', data=df)

sns.boxplot(x='Store_Size', y='Product_Store_Sales_Total', data=df)

sns.boxplot(x='Store_Location_City_Type', y='Product_Store_Sales_Total', data=df)

sns.boxplot(x='Store_Type', y='Product_Store_Sales_Total', data=df)
plt.xticks(rotation=45)

"""**Data Preprocessing**"""

# Define predictor matrix (X) using selected numeric and categorical features
X = df[numeric_features + categorical_features]

# Define target variable
y = df[target]

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y,              # Predictors (X) and target variable (y)
    test_size=0.2,     # 20% of the data is reserved for testing
    random_state=42    # Ensures reproducibility by setting a fixed random seed
)

preprocessor = make_column_transformer(
    (Pipeline([('num_imputer', SimpleImputer(strategy='median')),
               ('scaler', StandardScaler())]), numeric_features),
    (Pipeline([('cat_imputer', SimpleImputer(strategy='most_frequent')),
               ('encoder', OneHotEncoder(handle_unknown='ignore'))]), categorical_features)
)

"""**Model Building**"""

# function to compute adjusted R-squared
def adj_r2_score(predictors, targets, predictions):
    r2 = r2_score(targets, predictions)
    n = predictors.shape[0]
    k = predictors.shape[1]
    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))


# function to compute different metrics to check performance of a regression model
def model_performance_regression(model, predictors, target):
    """
    Function to compute different metrics to check regression model performance

    model: regressor
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    r2 = r2_score(target, pred)  # to compute R-squared
    adjr2 = adj_r2_score(predictors, target, pred)  # to compute adjusted R-squared
    rmse = np.sqrt(mean_squared_error(target, pred))  # to compute RMSE
    mae = mean_absolute_error(target, pred)  # to compute MAE
    mape = mean_absolute_percentage_error(target, pred)  # to compute MAPE

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "RMSE": rmse,
            "MAE": mae,
            "R-squared": r2,
            "Adj. R-squared": adjr2,
            "MAPE": mape,
        },
        index=[0],
    )

    return df_perf

"""**Random Forest Regressor**"""

# Define base Random Forest model
rf_model = RandomForestRegressor(random_state=42)

# Create pipeline with preprocessing and Random Forest model
rf_pipeline = make_pipeline(preprocessor, rf_model)

# Train the model pipeline on the training data
rf_pipeline.fit(X_train, y_train)

rf_estimator_model_train_perf = model_performance_regression(rf_pipeline, X_train,y_train)
print("Training performance \n")
rf_estimator_model_train_perf

rf_estimator_model_test_perf = model_performance_regression(rf_pipeline, X_test,y_test)
print("Testing performance \n")
rf_estimator_model_test_perf

"""**Random Forest Regressor**"""

# Choose the type of classifier.
rf_tuned = RandomForestRegressor(random_state=42)

# Create pipeline with preprocessing and XGBoost model
rf_pipeline = make_pipeline(preprocessor, rf_tuned)

# Grid of parameters to choose from
parameters = parameters = {
    'randomforestregressor__max_depth':[3, 4, 5, 6],
    'randomforestregressor__max_features': ['sqrt','log2',None],
    'randomforestregressor__n_estimators': [50, 75, 100, 125, 150]
}

# Type of scoring used to compare parameter combinations
scorer = metrics.make_scorer(metrics.r2_score)

# Run the grid search
grid_obj = GridSearchCV(rf_pipeline, parameters, scoring=scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
rf_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data.
rf_tuned.fit(X_train, y_train)

print(grid_obj.best_params_)

rf_tuned_model_train_perf = model_performance_regression(rf_tuned, X_train, y_train)
print("Training performance \n")
rf_tuned_model_train_perf

rf_tuned_model_test_perf = model_performance_regression(rf_tuned, X_test, y_test)
print("Testing performance \n")
rf_tuned_model_test_perf

"""**XGBoost Regressor**"""

# Define base XGBoost model
xgb_model = XGBRegressor(random_state=42)

# Create pipeline with preprocessing and XGBoost model
xgb_pipeline = make_pipeline(preprocessor, xgb_model)

# Train the model pipeline on the training data
xgb_pipeline.fit(X_train, y_train)

xgb_estimator_model_train_perf = model_performance_regression(xgb_pipeline, X_train, y_train)
print("Training performance \n")
xgb_estimator_model_train_perf

xgb_estimator_model_test_perf = model_performance_regression(xgb_pipeline, X_test,y_test)
print("Testing performance \n")
xgb_estimator_model_test_perf

"""**XGBoost Regressor - Hyperparameter Tuning**"""

# Choose the type of classifier.
xgb_tuned = XGBRegressor(random_state=42)

# Create pipeline with preprocessing and XGBoost model
xgb_pipeline = make_pipeline(preprocessor, xgb_tuned)

#Grid of parameters to choose from
param_grid = {
    'xgbregressor__n_estimators': [50, 100, 150, 200],    # number of trees to build
    'xgbregressor__max_depth': [2, 3, 4],    # maximum depth of each tree
    'xgbregressor__colsample_bytree': [0.4, 0.5, 0.6],    # percentage of attributes to be considered (randomly) for each tree
    'xgbregressor__colsample_bylevel': [0.4, 0.5, 0.6],    # percentage of attributes to be considered (randomly) for each level of a tree
    'xgbregressor__learning_rate': [0.01, 0.05, 0.1],    # learning rate
    'xgbregressor__reg_lambda': [0.4, 0.5, 0.6],    # L2 regularization factor
}

# Type of scoring used to compare parameter combinations
scorer = metrics.make_scorer(metrics.r2_score)

# Run the grid search
grid_obj = GridSearchCV(xgb_pipeline, param_grid, scoring=scorer,cv=5,n_jobs=-1)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
xgb_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data.
xgb_tuned.fit(X_train, y_train)

xgb_tuned_model_train_perf = model_performance_regression(xgb_tuned, X_train, y_train)
print("Training performance \n")
xgb_tuned_model_train_perf

xgb_tuned_model_test_perf = model_performance_regression(xgb_tuned, X_test, y_test)
print("Testing performance \n")
xgb_tuned_model_test_perf

"""**Model Performance Comparison and Final Model Selection**"""

# training performance comparison

models_train_comp_df = pd.concat(
    [rf_estimator_model_train_perf.T,rf_tuned_model_train_perf.T,
    xgb_estimator_model_train_perf.T,xgb_tuned_model_train_perf.T],
    axis=1,
)

models_train_comp_df.columns = [
    "Random Forest Estimator",
    "Random Forest Tuned",
    "XGBoost",
    "XGBoost Tuned"
]

print("Training performance comparison:")
models_train_comp_df

# Testing performance comparison

models_test_comp_df = pd.concat(
    [rf_estimator_model_test_perf.T,rf_tuned_model_test_perf.T,
    xgb_estimator_model_test_perf.T,xgb_tuned_model_test_perf.T],
    axis=1,
)

models_test_comp_df.columns = [
    "Random Forest Estimator",
    "Random Forest Tuned",
    "XGBoost",
    "XGBoost Tuned"
]

print("Testing performance comparison:")
models_test_comp_df

(models_train_comp_df - models_test_comp_df).iloc[2]

"""**Model Serialization**"""

# Create a folder for storing the files needed for web app deployment
os.makedirs("backend_files", exist_ok=True)

# Define the file path to save (serialize) the trained model along with the data preprocessing steps
saved_model_path = "backend_files/product_sales_prediction_model_v1_0.joblib"

# Save the best trained model pipeline using joblib
joblib.dump(rf_tuned, saved_model_path)

print(f"Model saved successfully at {saved_model_path}")

# Load the saved model pipeline from the file
saved_model = joblib.load("backend_files/product_sales_prediction_model_v1_0.joblib")

# Confirm the model is loaded
print("Model loaded successfully.")

# Printing saved model
saved_model

"""**Making Predictions Using the Deserialized Model**"""

saved_model.predict(X_test)

"""**Deployment**

**Hugging Face Docker Space**
"""

# API request
import requests

# Import the login function from the huggingface_hub library
from huggingface_hub import login

# Login to your Hugging Face account using your access token
login(token="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")

# Import the create_repo function from the huggingface_hub library
from huggingface_hub import create_repo

try:
    create_repo("superkart-backend",
        repo_type="space",
        space_sdk="docker",
        private=False
    )
except Exception as e:
    if "RepositoryAlreadyExistsError" in str(e) or "409" in str(e):
        print("Repository already exists. Skipping creation.")
    else:
        print(f"Error creating repository: {e}")

"""**Flask Web Framework**"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile backend_files/app.py
# import joblib
# import numpy as np
# import pandas as pd
# from flask import Flask, request, jsonify  # For creating the Flask API
# 
# # Initialize the Flask application
# product_sales_predictor_api = Flask("Product Sales Price Predictor")
# 
# # Load the trained machine learning model
# model = joblib.load("backend_files/product_sales_prediction_model_v1_0.joblib")
# 
# 
# # Define a route for the home page (GET request)
# @product_sales_predictor_api.get('/')
# def home():
#     """
#     This function handles GET requests to the root URL ('/') of the API.
#     It returns a simple welcome message.
#     """
#     return "Welcome to the Product Sales Prediction API!"
# 
# # Define an endpoint for single property prediction (POST request)
# @product_sales_predictor_api.post('/v1/psales')
# def predict_product_sales():
#     """
#     This function handles POST requests to the '/v1/psales' endpoint.
#     It expects a JSON payload containing store details and returns
#     the predicted product sales price as a JSON response.
#     """
#     # Get the JSON data from the request body
#     product_sales = request.get_json()
# 
#     # Extract relevant features from the JSON data
#     sample = {
#         'Product_Weight': product_sales['Product_Weight'],
#         'Product_Sugar_Content': product_sales['Product_Sugar_Content'],
#         'Product_Allocated_Area': product_sales['Product_Allocated_Area'],
#         'Product_Type': product_sales['Product_Type'],
#         'Product_MRP': product_sales['Product_MRP'],
#         'Store_Id': product_sales['Store_Id'],
#         'Store_Establishment_Year': product_sales['Store_Establishment_Year'],
#         'Store_Size': product_sales['Store_Size'],
#         'Store_Location_City_Type': product_sales['Store_Location_City_Type'],
#         'Store_Type': product_sales['Store_Type'],
# 
#     }
# 
#     # Convert the extracted data into a Pandas DataFrame
#     input_data = pd.DataFrame([sample])
# 
#     # Make prediction (get log_price)
#     predicted_price = model.predict(input_data)[0]
# 
#     # Convert predicted_price to Python float
#     predicted_price = round(float(predicted_price), 2)
#     # The conversion above is needed as we convert the model prediction (log price) to actual price using np.exp, which returns predictions as NumPy float32 values.
#     # When we send this value directly within a JSON response, Flask's jsonify function encounters a datatype error
# 
#     # Return the actual price
#     return jsonify({'Predicted Price (in dollars)': predicted_price})
# 
# 
# # Define an endpoint for batch prediction (POST request)
# @product_sales_predictor_api.post('/v1/psalesbatch')
# def predict_product_sales_batch():
#     """
#     This function handles POST requests to the '/v1/psalesbatch' endpoint.
#     It expects a CSV file containing property details for multiple stores
#     and returns the predicted product sales prices as a dictionary in the JSON response.
#     """
#     # Get the uploaded CSV file from the request
#     file = request.files['file']
# 
#     # Read the CSV file into a Pandas DataFrame
#     input_data = pd.read_csv(file)
# 
#     # Make predictions for all properties in the DataFrame (get log_prices)
#     predicted_prices = [round(float(price),2) for price in model.predict(input_data).tolist()]
# 
#     # Create a dictionary of predictions with property IDs as keys
#     product_ids = input_data['Product_Id'].tolist()
#     # Assuming 'id' is the property ID column
#     output_dict = dict(zip(product_ids, predicted_prices))  # Use actual prices
# 
#     # Return the predictions dictionary as a JSON response
#     return output_dict
# 
# # Run the Flask application in debug mode if this script is executed directly
# if __name__ == '__main__':
#     product_sales_predictor_api.run(host='0.0.0.0', port=8501, debug=True)

"""**Dependencies File**"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile backend_files/requirements.txt
# pandas==2.2.2
# numpy==1.26.4
# scikit-learn==1.5.2
# 
# xgboost==2.1.4
# joblib==1.4.2
# Werkzeug==2.2.2
# flask==2.2.2
# gunicorn==20.1.0
# requests==2.28.1
# uvicorn[standard]
# streamlit==1.43.2

"""**Dockerfile**"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile backend_files/Dockerfile
# FROM python:3.9-slim
# 
# # Set the working directory inside the container
# WORKDIR /app
# 
# # Copy all files from the current directory to the container's working directory
# COPY . .
# 
# # Install dependencies from the requirements file without using cache to reduce image size
# RUN pip install --no-cache-dir --upgrade -r requirements.txt
# 
# # Define the command to start the application using Gunicorn with 4 worker processes
# # - `-w 4`: Uses 4 worker processes for handling requests
# # - `-b 0.0.0.0:7860`: Binds the server to port 7860 on all network interfaces
# # - `app:product_sales_predictor_api`: Runs the Flask app (assuming `app.py` contains the Flask instance named `product_sales_predictor_api`)
# CMD ["gunicorn", "-w", "4", "-b", "0.0.0.0:7860", "app:product_sales_predictor_api"]

import os

print(os.listdir("backend_files"))

"""**Uploading Files to Hugging Face**"""

# for hugging face space authentication to upload files
from huggingface_hub import HfApi

repo_id = "vinay9700/superkart-backend"
  # Your Hugging Face space id

# Initialize the API
api = HfApi()

# Upload Streamlit app files stored in the folder called deployment_files
api.upload_folder(
    folder_path="/content/backend_files",  # Local folder path
    repo_id=repo_id,  # Hugging face space id
    repo_type="space",  # Hugging face repo type "space"
)

"""**Deployment**"""

# Create a folder for storing the files needed for frontend UI deployment
os.makedirs("frontend_files", exist_ok=True)

# Commented out IPython magic to ensure Python compatibility.
# %%writefile frontend_files/app.py
# 
# import streamlit as st
# import pandas as pd
# import joblib
# import requests
# 
# 
# # Streamlit UI for Customer Churn Prediction
# st.title("Product Sales Prediction App")
# st.write("The Product Sales Prediction App is a tool for businesses to forecast future sales and plan operations more effectively.")
# st.write("Kindly enter the product details to predict the expected sales revenue")
# 
# # Collect user input
# ProductWeight = st.number_input("Product Weight (weight of the product)", min_value=4.0, max_value=22.0, value=12.6)
# ProductSugarContent = st.selectbox("Product Sugar Content(sugar content of the product)", ["Low Sugar", "Regular", "No sugar"])
# ProductAllocatedArea = st.number_input("Product Allocated Area (ratio of the allocated display area of each product to the total display area )", min_value=0.00, max_value=0.29, value=0.06)
# ProductType = st.selectbox("Product Type", ['Baking Goods', 'Breads', 'Breakfast', 'Canned',
#                                               'Dairy', 'Frozen Food', 'Fruits and Vegetables',
#                                               'Hard Drinks', 'Health and Hygiene', 'Household',
#                                               'Meat', 'Others', 'Seafood', 'Snack foods',
#                                               'Soft Drinks', 'Starchy Foods'])
# ProductMRP = st.number_input("Product MRP(maximum retail price of each product)", min_value=31.0, max_value=266.0, value=141.0)
# StoreID = st.selectbox("Store Id",['OUT001', 'OUT002', 'OUT003', 'OUT004'] )
# StoreEstablishmentYear = st.number_input("Store Establishment Year (the year in which the store was established)", min_value=1987, max_value=2009, value=2000)
# StoreSize= st.selectbox("Store Size", ['Small','Medium', 'High'])
# StoreLocationCityType = st.selectbox("Store Location City Type", ['Tier 1', 'Tier 2', 'Tier 3'])
# StoreType = st.selectbox("Store Type",['Departmental Store', 'Food Mart', 'Supermarket Type 1', 'Supermarket Type 2'])
# 
# # Convert categorical inputs to match model training
# # Syntax
# #input_data = pd.DataFrame([{'datasetvariable1': userinputvariable1,}])
# input_data = pd.DataFrame([{
#     'Product_Weight': ProductWeight,
#     'Product_Sugar_Content': ProductSugarContent,
#     'Product_Allocated_Area': ProductAllocatedArea,
#     'Product_Type': ProductType,
#     'Product_MRP': ProductMRP,
#     'Store_Id': StoreID,
#     'Store_Establishment_Year': StoreEstablishmentYear,
#     'Store_Size': StoreSize,
#     'Store_Location_City_Type': StoreLocationCityType,
#     'Store_Type': StoreType
# }])
# 
# if st.button("Predict"):
#     headers = {'Content-Type': 'application/json'}
#     response = requests.post("https://sujatasuren-superkart-backend.hf.space/v1/psales", json=input_data.to_dict(orient='records')[0], headers=headers)  # Send data to Flask API
#     if response.status_code == 200:
#         prediction = response.json()['Predicted Price (in dollars)']
#         st.success(f"The predicted product sales revenue is $ {prediction}")
#     else:
#         st.error("Error in API request")
#         st.write(f"Status Code: {response.status_code}")
#         st.write(f"Response Text: {response.text}")
# 
# # Section for batch prediction
# st.subheader("Batch Prediction")
# 
# # Allow users to upload a CSV file for batch prediction
# uploaded_file = st.file_uploader("Upload CSV file for batch prediction", type=["csv"])
# 
# # Make batch prediction when the "Predict Batch" button is clicked
# if uploaded_file is not None:
#     if st.button("Predict Batch"):
#         response = requests.post("https://sujatasuren-superkart-backend.hf.space/v1/psalesbatch", files={"file": uploaded_file})  # Send file to Flask API
#         if response.status_code == 200:
#             predictions = response.json()
#             st.success("Batch predictions completed!")
#             st.write(predictions)  # Display the predictions
#         else:
#             st.error("Error making batch prediction.")

"""**Dependencies File**"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile frontend_files/requirements.txt
# pandas==2.2.2
# requests==2.28.1
# streamlit==1.43.2
# joblib==1.4.2

"""**DockerFile**"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile frontend_files/Dockerfile
# # Use a minimal base image with Python 3.9 installed
# FROM python:3.9-slim
# 
# # Set the working directory inside the container to /app
# WORKDIR /app
# 
# # Copy all files from the current directory on the host to the container's /app directory
# COPY . .
# 
# # Install Python dependencies listed in requirements.txt
# RUN pip3 install -r requirements.txt
# 
# # Define the command to run the Streamlit app on port 8501 and make it accessible externally
# CMD ["streamlit", "run", "app.py", "--server.port=8501", "--server.address=0.0.0.0", "--server.enableXsrfProtection=false"]
# 
# # NOTE: Disable XSRF protection for easier external access in order to make batch predictions

import os

if os.path.exists("frontend_files"):
    print(os.listdir("frontend_files"))
else:
    print("Folder 'frontend_files' does not exist.")

"""**Uploading Files to Hugging Face**"""

access_key = "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" # ðŸ”’ Replace with your actual token (Write/Admin)
repo_id = "vinay9700/superkart-frontend" # Hugging Face Space ID
# Login to Hugging Face platform with the access token
login(token=access_key)

# Initialize the API
api = HfApi()

# Upload Streamlit app files stored in the folder called deployment_files
api.upload_folder(
    folder_path="/content/frontend_files",  # Local folder path
    repo_id=repo_id,  # Hugging face space id
    repo_type="space",  # Hugging face repo type "space"
)

from huggingface_hub import HfApi, create_repo

repo_id = "vinay9700/superkart-backend"  # Your Hugging Face Space ID
api = HfApi()

try:
    # Check if the space exists
    api.repo_info(repo_id, repo_type="space")
    print(f"âœ… Repo '{repo_id}' already exists. Skipping creation.")
except Exception:
    print(f"âš¡ Creating new repo: {repo_id}")
    create_repo(
        repo_id,
        repo_type="space",
        space_sdk="docker",
        private=False
    )

# Now upload files
api.upload_folder(
    folder_path="backend_files",  # local folder with your code
    repo_id=repo_id,
    repo_type="space"
)

"""**Inferencing**"""

import json  # To handle JSON formatting for API requests and responses
import requests  # To send HTTP requests to the deployed Flask API

import pandas as pd  # For data manipulation and analysis
import numpy as np  # For numerical computations

model_root_url = "https://vinay9700-superkart-backend.hf.space/"

model_url = model_root_url + "/v1/psales"  # Endpoint for online (single) inference

model_batch_url = model_root_url + "/v1/psalesbatch"  # Endpoint for batch inference

payload ={
  "Product_Weight": 12.6,
  "Product_Sugar_Content": "Regulat",
  "Product_Allocated_Area": 0.06,
  "Product_Type": "Snack foods",
  "Product_MRP": 141.0,
  "Store_Id": "OUT003",
  "Store_Establishment_Year": 2002,
  "Store_Size": "Medium",
  "Store_Location_City_Type": "Tier 1",
  "Store_Type": "Supermarket Type 1"
}

# Sending a POST request to the model endpoint with the test payload
response = requests.post(model_url, json=payload)

response

response.json()

print(response.text)